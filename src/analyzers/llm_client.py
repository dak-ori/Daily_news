"""
LLM í´ë¼ì´ì–¸íŠ¸
ë‹¤ì–‘í•œ LLM APIë¥¼ í†µí•©í•˜ì—¬ ê´€ë¦¬í•©ë‹ˆë‹¤.
"""
from typing import Optional, Dict, Any
from dataclasses import dataclass, field
from datetime import datetime
import logging
from config.settings import settings

logger = logging.getLogger(__name__)


@dataclass
class TokenUsage:
    """í† í° ì‚¬ìš©ëŸ‰ ì¶”ì """
    prompt_tokens: int = 0
    completion_tokens: int = 0
    total_tokens: int = 0
    estimated_cost: float = 0.0
    timestamp: datetime = field(default_factory=datetime.now)
    model: str = ""
    provider: str = ""


class TokenMonitor:
    """í† í° ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§"""
    
    # ëª¨ë¸ë³„ ê°€ê²© (1K í† í°ë‹¹ USD)
    PRICING = {
        "gpt-4o-mini": {"input": 0.00015, "output": 0.0006},
        "gpt-4o": {"input": 0.005, "output": 0.015},
        "claude-3-haiku-20240307": {"input": 0.00025, "output": 0.00125},
        "claude-3-sonnet-20240229": {"input": 0.003, "output": 0.015},
        "gemini-2.5-flash": {"input": 0.0, "output": 0.0},  # ë¬´ë£Œ í‹°ì–´
    }
    
    def __init__(self):
        self.usage_history: list[TokenUsage] = []
        self.total_prompt_tokens = 0
        self.total_completion_tokens = 0
        self.total_cost = 0.0
    
    def record_usage(
        self,
        prompt_tokens: int,
        completion_tokens: int,
        model: str,
        provider: str
    ) -> TokenUsage:
        """í† í° ì‚¬ìš©ëŸ‰ ê¸°ë¡"""
        total = prompt_tokens + completion_tokens
        
        # ë¹„ìš© ê³„ì‚°
        pricing = self.PRICING.get(model, {"input": 0, "output": 0})
        cost = (prompt_tokens / 1000 * pricing["input"]) + \
               (completion_tokens / 1000 * pricing["output"])
        
        usage = TokenUsage(
            prompt_tokens=prompt_tokens,
            completion_tokens=completion_tokens,
            total_tokens=total,
            estimated_cost=cost,
            model=model,
            provider=provider
        )
        
        self.usage_history.append(usage)
        self.total_prompt_tokens += prompt_tokens
        self.total_completion_tokens += completion_tokens
        self.total_cost += cost
        
        logger.info(
            f"ğŸ“Š í† í° ì‚¬ìš©: {prompt_tokens}+{completion_tokens}={total} "
            f"(${cost:.6f}) | ëˆ„ì : {self.total_prompt_tokens}+{self.total_completion_tokens} "
            f"(${self.total_cost:.6f})"
        )
        
        return usage
    
    def get_summary(self) -> Dict[str, Any]:
        """ì‚¬ìš©ëŸ‰ ìš”ì•½"""
        return {
            "total_requests": len(self.usage_history),
            "total_prompt_tokens": self.total_prompt_tokens,
            "total_completion_tokens": self.total_completion_tokens,
            "total_tokens": self.total_prompt_tokens + self.total_completion_tokens,
            "total_cost_usd": round(self.total_cost, 6),
        }


class LLMClient:
    """LLM API í´ë¼ì´ì–¸íŠ¸"""
    
    # ì „ì—­ í† í° ëª¨ë‹ˆí„°
    token_monitor = TokenMonitor()

    def __init__(self, provider: str = "openai"):
        """
        LLM í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”

        Args:
            provider: 'openai', 'anthropic', 'google' ì¤‘ ì„ íƒ
        """
        self.provider = provider
        self._client = None
        self._initialize_client()

    def _initialize_client(self):
        """ì„ íƒí•œ providerì˜ í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”"""
        if self.provider == "openai":
            self._initialize_openai()
        elif self.provider == "anthropic":
            self._initialize_anthropic()
        elif self.provider == "google":
            self._initialize_google()
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” provider: {self.provider}")

    def _initialize_openai(self):
        """OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”"""
        try:
            from openai import OpenAI

            if not settings.openai_api_key:
                raise ValueError("OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

            self._client = OpenAI(api_key=settings.openai_api_key)
            logger.info("OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            logger.error(f"OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise

    def _initialize_anthropic(self):
        """Anthropic í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”"""
        try:
            from anthropic import Anthropic

            if not settings.anthropic_api_key:
                raise ValueError("ANTHROPIC_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

            self._client = Anthropic(api_key=settings.anthropic_api_key)
            logger.info("Anthropic í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            logger.error(f"Anthropic í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise

    def _initialize_google(self):
        """Google Gemini í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”"""
        try:
            import google.generativeai as genai

            if not settings.google_api_key:
                raise ValueError("GOOGLE_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

            genai.configure(api_key=settings.google_api_key)
            # gemini-2.5-flash ì‚¬ìš© (ìµœì‹  ëª¨ë¸)
            self._client = genai.GenerativeModel("models/gemini-2.5-flash")
            logger.info("Google Gemini 2.5 Flash í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            logger.error(f"Google Gemini í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise

    def generate(
        self, prompt: str, system_prompt: Optional[str] = None, **kwargs
    ) -> str:
        """í…ìŠ¤íŠ¸ ìƒì„±"""
        try:
            if self.provider == "openai":
                return self._generate_openai(prompt, system_prompt, **kwargs)
            elif self.provider == "anthropic":
                return self._generate_anthropic(prompt, system_prompt, **kwargs)
            elif self.provider == "google":
                return self._generate_google(prompt, system_prompt, **kwargs)
        except Exception as e:
            logger.error(f"í…ìŠ¤íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
            raise

    def _generate_openai(
        self, prompt: str, system_prompt: Optional[str] = None, **kwargs
    ) -> str:
        """OpenAIë¡œ í…ìŠ¤íŠ¸ ìƒì„±"""
        messages = []

        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})

        messages.append({"role": "user", "content": prompt})

        model = kwargs.get("model", "gpt-4o-mini")
        response = self._client.chat.completions.create(
            model=model,
            messages=messages,
            temperature=kwargs.get("temperature", 0.7),
            max_tokens=kwargs.get("max_tokens", 1000),
        )

        # í† í° ì‚¬ìš©ëŸ‰ ê¸°ë¡
        if response.usage:
            self.token_monitor.record_usage(
                prompt_tokens=response.usage.prompt_tokens,
                completion_tokens=response.usage.completion_tokens,
                model=model,
                provider="openai"
            )

        return response.choices[0].message.content

    def _generate_anthropic(
        self, prompt: str, system_prompt: Optional[str] = None, **kwargs
    ) -> str:
        """Anthropicìœ¼ë¡œ í…ìŠ¤íŠ¸ ìƒì„±"""
        model = kwargs.get("model", "claude-3-haiku-20240307")
        response = self._client.messages.create(
            model=model,
            max_tokens=kwargs.get("max_tokens", 1000),
            temperature=kwargs.get("temperature", 0.7),
            system=system_prompt or "",
            messages=[{"role": "user", "content": prompt}],
        )

        # í† í° ì‚¬ìš©ëŸ‰ ê¸°ë¡
        if response.usage:
            self.token_monitor.record_usage(
                prompt_tokens=response.usage.input_tokens,
                completion_tokens=response.usage.output_tokens,
                model=model,
                provider="anthropic"
            )

        return response.content[0].text

    def _generate_google(
        self, prompt: str, system_prompt: Optional[str] = None, **kwargs
    ) -> str:
        """Google Geminië¡œ í…ìŠ¤íŠ¸ ìƒì„±"""
        full_prompt = prompt
        if system_prompt:
            full_prompt = f"{system_prompt}\n\n{prompt}"

        response = self._client.generate_content(full_prompt)
        
        # Gemini í† í° ì‚¬ìš©ëŸ‰ ê¸°ë¡ (ì¶”ì •ì¹˜)
        # GeminiëŠ” ë¬´ë£Œ í‹°ì–´ì´ë¯€ë¡œ ëŒ€ëµì ì¸ í† í° ìˆ˜ë§Œ ì¶”ì •
        prompt_tokens = len(full_prompt.split()) * 1.3  # ëŒ€ëµì  ì¶”ì •
        completion_tokens = len(response.text.split()) * 1.3 if response.text else 0
        self.token_monitor.record_usage(
            prompt_tokens=int(prompt_tokens),
            completion_tokens=int(completion_tokens),
            model="gemini-2.5-flash",
            provider="google"
        )
        
        return response.text
    
    @classmethod
    def get_usage_summary(cls) -> Dict[str, Any]:
        """ì „ì—­ í† í° ì‚¬ìš©ëŸ‰ ìš”ì•½ ì¡°íšŒ"""
        return cls.token_monitor.get_summary()
